{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Crime Data Analyst (Extra) (Individual)\n",
    "### (For Big Data Analytics and Applications)\n",
    "\n",
    "## Scenario\n",
    "As a data science expert, you got a job from a police departement from another city. After you review the crime data, you are asksed to develop a prediction model to \n",
    "\n",
    "### Plan of attack:\n",
    "    \n",
    "1. **We want to use our skills to detect/predict crime in this city. Therefore, we want to build a classifier, which predicts the type of crime when it occurs. (100%)**\n",
    "    1. Load the training data for a machine learning classifier in _'data/crime_training_data.csv'_ and corresponding training labels in _'data/crime_training_labels.csv'_.\n",
    "    2. Train at least two kinds of classifiers with the training data set. \n",
    "    3. Predict the crime type (labels) of reported crimes in _'data/crime_test_data.csv'_.\n",
    "    4. Save the predictions in a _.csv_ file.\n",
    "    5. Compare the predicitons of your chosen classifiers and explain why results.\n",
    "\n",
    "### Hand in:\n",
    "Hand in the following files in a _.zip_ file:\n",
    "   - your code in a jupyter notebook (_.ipynb_) or standard python source code (_.py_).\n",
    "   - a _.pdf_ with your findings and plots (you can easily create a PDF in juypter notebook under ->File-> Download as-> PDF via Latex, might need to install some software)\n",
    "   - your predictions as a _csv._ file.\n",
    "   \n",
    "The files should have your name(s) or your studentID in them, ex: _homework1_extra_M112010001.ipynb_ , _homework1_extra_M112010001.py_, or _homework1_extra_M112010001.pdf_.\n",
    "\n",
    "Send the _.zip_ file to moodle system (moodle2.ntust.edu.tw) before **Tuesday March 4 2025 09:10 AM**.   \n",
    "\n",
    "### Note on implementation:\n",
    "- You are free to use any classification algorithm that you want. If you find better recommendation approaches on the web(there certainly are better, but also more involved ones), you are free to use those.\n",
    "- Some proven algorithms, other than the ones learned in the lectures are: \n",
    "    - https://towardsdatascience.com/collaborative-filtering-simplified-the-basic-science-behind-recommendation-systems-1d7e7c58cd8/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "training_data = pd.read_csv('data/crime_training_data.csv')\n",
    "training_labels = pd.read_csv('data/crime_training_labels.csv')\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv('data/crime_test_data.csv')\n",
    "test_labels = pd.read_csv('data/crime_test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = training_data.values\n",
    "y_train = training_labels.values.ravel()  # Convert labels to a 1D array\n",
    "X_test = test_data.values\n",
    "y_test = test_labels.values.ravel()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/120], Loss: 1.9295\n",
      "Epoch [2/120], Loss: 1.3930\n",
      "Epoch [3/120], Loss: 1.3331\n",
      "Epoch [4/120], Loss: 1.3434\n",
      "Epoch [5/120], Loss: 1.2638\n",
      "Epoch [6/120], Loss: 1.2281\n",
      "Epoch [7/120], Loss: 1.1607\n",
      "Epoch [8/120], Loss: 1.1305\n",
      "Epoch [9/120], Loss: 1.1163\n",
      "Epoch [10/120], Loss: 1.1033\n",
      "Epoch [11/120], Loss: 1.0768\n",
      "Epoch [12/120], Loss: 1.0731\n",
      "Epoch [13/120], Loss: 1.0219\n",
      "Epoch [14/120], Loss: 1.0555\n",
      "Epoch [15/120], Loss: 1.0231\n",
      "Epoch [16/120], Loss: 0.9882\n",
      "Epoch [17/120], Loss: 0.9571\n",
      "Epoch [18/120], Loss: 1.0023\n",
      "Epoch [19/120], Loss: 0.9424\n",
      "Epoch [20/120], Loss: 0.9593\n",
      "Epoch [21/120], Loss: 0.9354\n",
      "Epoch [22/120], Loss: 0.9171\n",
      "Epoch [23/120], Loss: 0.8971\n",
      "Epoch [24/120], Loss: 0.9000\n",
      "Epoch [25/120], Loss: 0.9089\n",
      "Epoch [26/120], Loss: 0.9451\n",
      "Epoch [27/120], Loss: 0.9328\n",
      "Epoch [28/120], Loss: 0.9235\n",
      "Epoch [29/120], Loss: 0.9026\n",
      "Epoch [30/120], Loss: 0.8995\n",
      "Epoch [31/120], Loss: 0.9010\n",
      "Epoch [32/120], Loss: 0.9075\n",
      "Epoch [33/120], Loss: 0.8635\n",
      "Epoch [34/120], Loss: 0.9073\n",
      "Epoch [35/120], Loss: 0.8862\n",
      "Epoch [36/120], Loss: 0.8477\n",
      "Epoch [37/120], Loss: 0.9171\n",
      "Epoch [38/120], Loss: 0.8366\n",
      "Epoch [39/120], Loss: 0.8525\n",
      "Epoch [40/120], Loss: 0.8448\n",
      "Epoch [41/120], Loss: 0.8427\n",
      "Epoch [42/120], Loss: 0.8172\n",
      "Epoch [43/120], Loss: 0.8343\n",
      "Epoch [44/120], Loss: 0.8336\n",
      "Epoch [45/120], Loss: 0.8261\n",
      "Epoch [46/120], Loss: 0.8776\n",
      "Epoch [47/120], Loss: 0.8421\n",
      "Epoch [48/120], Loss: 0.8208\n",
      "Epoch [49/120], Loss: 0.8178\n",
      "Epoch [50/120], Loss: 0.8330\n",
      "Epoch [51/120], Loss: 0.8628\n",
      "Epoch [52/120], Loss: 0.8123\n",
      "Epoch [53/120], Loss: 0.8128\n",
      "Epoch [54/120], Loss: 0.8486\n",
      "Epoch [55/120], Loss: 0.8453\n",
      "Epoch [56/120], Loss: 0.8360\n",
      "Epoch [57/120], Loss: 0.8086\n",
      "Epoch [58/120], Loss: 0.7903\n",
      "Epoch [59/120], Loss: 0.8118\n",
      "Epoch [60/120], Loss: 0.7858\n",
      "Epoch [61/120], Loss: 0.8151\n",
      "Epoch [62/120], Loss: 0.7916\n",
      "Epoch [63/120], Loss: 0.7918\n",
      "Epoch [64/120], Loss: 0.7849\n",
      "Epoch [65/120], Loss: 0.8197\n",
      "Epoch [66/120], Loss: 0.8052\n",
      "Epoch [67/120], Loss: 0.8001\n",
      "Epoch [68/120], Loss: 0.7867\n",
      "Epoch [69/120], Loss: 0.7929\n",
      "Epoch [70/120], Loss: 0.8006\n",
      "Epoch [71/120], Loss: 0.7688\n",
      "Epoch [72/120], Loss: 0.7692\n",
      "Epoch [73/120], Loss: 0.7630\n",
      "Epoch [74/120], Loss: 0.7901\n",
      "Epoch [75/120], Loss: 0.7926\n",
      "Epoch [76/120], Loss: 0.7766\n",
      "Epoch [77/120], Loss: 0.7729\n",
      "Epoch [78/120], Loss: 0.7679\n",
      "Epoch [79/120], Loss: 0.7732\n",
      "Epoch [80/120], Loss: 0.7851\n",
      "Epoch [81/120], Loss: 0.7709\n",
      "Epoch [82/120], Loss: 0.7982\n",
      "Epoch [83/120], Loss: 0.7888\n",
      "Epoch [84/120], Loss: 0.7710\n",
      "Epoch [85/120], Loss: 0.7730\n",
      "Epoch [86/120], Loss: 0.7590\n",
      "Epoch [87/120], Loss: 0.7786\n",
      "Epoch [88/120], Loss: 0.7478\n",
      "Epoch [89/120], Loss: 0.7733\n",
      "Epoch [90/120], Loss: 0.7659\n",
      "Epoch [91/120], Loss: 0.7582\n",
      "Epoch [92/120], Loss: 0.7429\n",
      "Epoch [93/120], Loss: 0.7679\n",
      "Epoch [94/120], Loss: 0.7886\n",
      "Epoch [95/120], Loss: 0.7518\n",
      "Epoch [96/120], Loss: 0.7443\n",
      "Epoch [97/120], Loss: 0.7676\n",
      "Epoch [98/120], Loss: 0.7566\n",
      "Epoch [99/120], Loss: 0.7666\n",
      "Epoch [100/120], Loss: 0.7659\n",
      "Epoch [101/120], Loss: 0.7479\n",
      "Epoch [102/120], Loss: 0.7390\n",
      "Epoch [103/120], Loss: 0.7545\n",
      "Epoch [104/120], Loss: 0.7660\n",
      "Epoch [105/120], Loss: 0.7953\n",
      "Epoch [106/120], Loss: 0.7752\n",
      "Epoch [107/120], Loss: 0.7278\n",
      "Epoch [108/120], Loss: 0.7777\n",
      "Epoch [109/120], Loss: 0.7947\n",
      "Epoch [110/120], Loss: 0.7369\n",
      "Epoch [111/120], Loss: 0.7375\n",
      "Epoch [112/120], Loss: 0.7518\n",
      "Epoch [113/120], Loss: 0.7444\n",
      "Epoch [114/120], Loss: 0.7582\n",
      "Epoch [115/120], Loss: 0.7744\n",
      "Epoch [116/120], Loss: 0.7246\n",
      "Epoch [117/120], Loss: 0.7386\n",
      "Epoch [118/120], Loss: 0.7533\n",
      "Epoch [119/120], Loss: 0.7443\n",
      "Epoch [120/120], Loss: 0.7480\n",
      "Test Accuracy: 58.17%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CrimeClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_classes=4):\n",
    "        super(CrimeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x)) \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "model = CrimeClassifier(input_size=input_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 120\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 65.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assume training_data, training_labels, test_data, test_labels are already defined\n",
    "\n",
    "# Encode labels into numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(training_labels.values.ravel())\n",
    "y_test_encoded = label_encoder.transform(test_labels.values.ravel())\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(training_data.values, y_train_encoded)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(test_data.values)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
